{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: All necessary imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "                                               \n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (0.3.11)\n",
      "Requirement already satisfied: packaging in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\envs\\neu_csye_7374_gpu\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "KaggleApiHTTPError",
     "evalue": "404 Client Error.\n\nResource not found at URL: https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university/versions/2\nThe server reported the following issues: Dataset not found\nPlease make sure you specified the correct resource identifiers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\kagglehub\\exceptions.py:66\u001b[0m, in \u001b[0;36mkaggle_api_raise_for_status\u001b[1;34m(response, resource_handle)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://www.kaggle.com/api/v1/datasets/download/raddar/chest-xrays-indiana-university?dataset_version_number=2&file_name=c:%5CUsers%5CChris%5Canaconda3%5Cenvs%5CNEU_CSYE_7374_GPU%5CNEU_CSYE_7374_Assignments%5CFinal%5CData",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKaggleApiHTTPError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(data_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Download the dataset from Kaggle\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraddar/chest-xrays-indiana-university\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to dataset files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\kagglehub\\datasets.py:35\u001b[0m, in \u001b[0;36mdataset_download\u001b[1;34m(handle, path, force_download)\u001b[0m\n\u001b[0;32m     33\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_dataset_handle(handle)\n\u001b[0;32m     34\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[1;32m---> 35\u001b[0m path, _ \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\kagglehub\\registry.py:28\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\kagglehub\\resolver.py:29\u001b[0m, in \u001b[0;36mResolver.__call__\u001b[1;34m(self, handle, path, force_download)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     path, version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     register_datasource_access(handle, version)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\kagglehub\\http_resolver.py:122\u001b[0m, in \u001b[0;36mDatasetHttpResolver._resolve\u001b[1;34m(self, h, path, force_download)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# Downloading a single file.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(out_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 122\u001b[0m     \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_auto_compressed_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# TODO(b/345800027) Implement parallel download when < 25 files in databundle.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Downloading the full archived bundle.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     archive_path \u001b[38;5;241m=\u001b[39m get_cached_archive_path(h)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\kagglehub\\clients.py:181\u001b[0m, in \u001b[0;36mKaggleApiV1Client.download_file\u001b[1;34m(self, path, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[0m\n\u001b[0;32m    173\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_url(path)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    175\u001b[0m     url,\n\u001b[0;32m    176\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_user_agent()},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m--> 181\u001b[0m     \u001b[43mkaggle_api_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     size_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\kagglehub\\exceptions.py:106\u001b[0m, in \u001b[0;36mkaggle_api_raise_for_status\u001b[1;34m(response, resource_handle)\u001b[0m\n\u001b[0;32m     97\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m     )\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Default handling\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m KaggleApiHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mKaggleApiHTTPError\u001b[0m: 404 Client Error.\n\nResource not found at URL: https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university/versions/2\nThe server reported the following issues: Dataset not found\nPlease make sure you specified the correct resource identifiers."
     ]
    }
   ],
   "source": [
    "# Download the dataset - Chest X-rays (IU)\n",
    "# Download latest version\n",
    "import kagglehub\n",
    "import os\n",
    "import urllib.request\n",
    "import pathlib\n",
    "\n",
    "%pip install --upgrade kagglehub\n",
    "\n",
    "# Get current working directory\n",
    "parent_dir = pathlib.Path(os.getcwd())\n",
    "\n",
    "# Define the path to the data directory\n",
    "data_dir = parent_dir / \"Data\"\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download the dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"raddar/chest-xrays-indiana-university\", path=data_dir)\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIT1T2Dataset(Dataset):\n",
    "    \"\"\"Dataset class for loading paired/unpaired T1-T2 MRI data.\n",
    "    Handles loading, validation, normalization and caching of MRI volumes.\"\"\"\n",
    "    \n",
    "    def __init__(self, t1_dir, t2_dir, slice_mode='middle', paired=True, transform=None, cache_size=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t1_dir (str): Directory containing T1 scans\n",
    "            t2_dir (str): Directory containing T2 scans\n",
    "            slice_mode (str): 'middle' or 'random' - how to select slice from volume\n",
    "            paired (bool): If True, uses paired T1-T2 data, else random unpaired selection\n",
    "            transform: Optional transforms to apply to slices\n",
    "            cache_size (int): Number of volumes to cache in memory (0 for no caching)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add resize transform\n",
    "        self.resize_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((128, 128)), \n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # Store initialization parameters\n",
    "        self.t1_dir = t1_dir\n",
    "        self.t2_dir = t2_dir\n",
    "        self.transform = transform\n",
    "        self.slice_mode = slice_mode\n",
    "        self.paired = paired\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        # Get lists of all NIfTI files (.nii.gz)\n",
    "        self.t1_files = sorted([f for f in os.listdir(t1_dir) if f.endswith('.nii.gz')])\n",
    "        self.t2_files = sorted([f for f in os.listdir(t2_dir) if f.endswith('.nii.gz')])\n",
    "        \n",
    "        # For paired training, find matching T1-T2 pairs based on subject ID\n",
    "        if self.paired:\n",
    "            self.paired_files = []\n",
    "            for t1f in self.t1_files:\n",
    "                subject_id = t1f.split('-')[0][3:]  # Extract subject ID from filename\n",
    "                matching_t2 = [t2f for t2f in self.t2_files if t2f.split('-')[0][3:] == subject_id]\n",
    "                if matching_t2:\n",
    "                    self.paired_files.append((t1f, matching_t2[0]))\n",
    "            print(f\"Found {len(self.paired_files)} paired T1/T2 datasets\")\n",
    "            self.data_files = self.paired_files\n",
    "        else:\n",
    "            # For unpaired, just use T1 files and randomly select T2 later\n",
    "            self.data_files = [(t1f, None) for t1f in self.t1_files]\n",
    "        \n",
    "        # Initialize cache dictionary\n",
    "        self.cache = {}\n",
    "\n",
    "    def _load_and_validate_volume(self, filename, is_t1=True):\n",
    "        \"\"\"Load and validate a single MRI volume.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Name of the NIfTI file\n",
    "            is_t1 (bool): Whether this is a T1 volume (determines directory)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (volume data array, (min_value, max_value))\n",
    "        \"\"\"\n",
    "        dir_path = self.t1_dir if is_t1 else self.t2_dir\n",
    "        filepath = os.path.join(dir_path, filename)\n",
    "        \n",
    "        # Check if volume is in cache\n",
    "        if filepath in self.cache:\n",
    "            vol = self.cache[filepath]\n",
    "        else:\n",
    "            # Load volume using NiBabel\n",
    "            vol = nib.load(filepath).get_fdata()\n",
    "            \n",
    "            # Validate volume\n",
    "            if not self.is_valid_volume(vol):\n",
    "                raise ValueError(f\"Invalid volume: {filename}\")\n",
    "            \n",
    "            # Cache volume if cache isn't full\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[filepath] = vol\n",
    "    \n",
    "        # Calculate volume statistics for normalization\n",
    "        stats = (float(vol.min()), float(vol.max()))\n",
    "        \n",
    "        return vol, stats\n",
    "\n",
    "    def is_valid_volume(self, vol):\n",
    "        \"\"\"Check if volume meets quality criteria.\n",
    "        \n",
    "        Args:\n",
    "            vol (np.ndarray): Volume data\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if volume is valid\n",
    "        \"\"\"\n",
    "        min_size = 64  # Minimum dimension size\n",
    "        return (vol.shape[0] >= min_size and \n",
    "                vol.shape[1] >= min_size and \n",
    "                vol.shape[2] >= 1 and \n",
    "                not np.any(np.isnan(vol)))\n",
    "\n",
    "    def get_slice_idx(self, volume):\n",
    "        \"\"\"Get slice index based on slice_mode setting.\n",
    "        \n",
    "        Args:\n",
    "            volume (np.ndarray): Volume data\n",
    "            \n",
    "        Returns:\n",
    "            int: Index of slice to extract\n",
    "        \"\"\"\n",
    "        if self.slice_mode == 'middle':\n",
    "            return volume.shape[2] // 2\n",
    "        else:  # random\n",
    "            return random.randint(0, volume.shape[2] - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of T1-T2 pairs in the dataset.\"\"\"\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a T1-T2 pair of slices.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the pair\n",
    "            \n",
    "        Returns:\n",
    "            dict: Contains 'T1' and 'T2' tensor slices\n",
    "        \"\"\"\n",
    "        t1_file, t2_file = self.data_files[idx]\n",
    "        \n",
    "        # Load T1 volume and its statistics\n",
    "        t1_vol, t1_stats = self._load_and_validate_volume(t1_file, is_t1=True)\n",
    "        \n",
    "        if self.paired:\n",
    "            # Load matching T2 volume for paired data\n",
    "            t2_vol, t2_stats = self._load_and_validate_volume(t2_file, is_t1=False)\n",
    "        else:\n",
    "            # For unpaired data, randomly select a T2 volume\n",
    "            random_t2_idx = random.randint(0, len(self.t2_files) - 1)\n",
    "            t2_vol, t2_stats = self._load_and_validate_volume(self.t2_files[random_t2_idx], is_t1=False)\n",
    "        \n",
    "        # Determine slice indices independently based on slice_mode\n",
    "        if self.slice_mode == 'middle':\n",
    "            t1_slice_idx = t1_vol.shape[2] // 2\n",
    "            t2_slice_idx = t2_vol.shape[2] // 2\n",
    "        else:  # 'random'\n",
    "            t1_slice_idx = random.randint(0, t1_vol.shape[2] - 1)\n",
    "            t2_slice_idx = random.randint(0, t2_vol.shape[2] - 1)\n",
    "        \n",
    "        # Extract and normalize the slices\n",
    "        t1_slice = self.normalize_slice(t1_vol[:, :, t1_slice_idx], t1_stats)\n",
    "        t2_slice = self.normalize_slice(t2_vol[:, :, t2_slice_idx], t2_stats)\n",
    "        \n",
    "        # Convert slices to tensors and add a channel dimension\n",
    "        t1_tensor = torch.from_numpy(t1_slice).float().unsqueeze(0)\n",
    "        t2_tensor = torch.from_numpy(t2_slice).float().unsqueeze(0)\n",
    "        \n",
    "        # Apply the resize transform (e.g., to 128x128)\n",
    "        t1_tensor = self.resize_transform(t1_tensor)\n",
    "        t2_tensor = self.resize_transform(t2_tensor)\n",
    "        \n",
    "        # Apply any additional transforms if provided\n",
    "        if self.transform:\n",
    "            t1_tensor = self.transform(t1_tensor)\n",
    "            t2_tensor = self.transform(t2_tensor)\n",
    "        \n",
    "        # Clean up volumes if not cached\n",
    "        if t1_file not in self.cache:\n",
    "            del t1_vol\n",
    "        if t2_file not in self.cache:\n",
    "            del t2_vol\n",
    "\n",
    "        return {'T1': t1_tensor, 'T2': t2_tensor}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_slice(slice_data, stats):\n",
    "        \"\"\"Normalize slice to [0,1] range using pre-computed statistics.\n",
    "        \n",
    "        Args:\n",
    "            slice_data (np.ndarray): Raw slice data\n",
    "            stats (tuple): (min_value, max_value) for normalization\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Normalized slice\n",
    "        \"\"\"\n",
    "        min_val, max_val = stats\n",
    "        return (slice_data - min_val) / (max_val - min_val + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (U-net with Time + Cross-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define an Identity module to bypass attention when not used.\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Projects timesteps into a higher-dimensional space for time conditioning.\"\"\"\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.time_proj = nn.Sequential(\n",
    "            nn.Linear(1, n_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(n_channels, n_channels)\n",
    "        )\n",
    "    def forward(self, t):\n",
    "        # t: [B] -> [B, 1]\n",
    "        t = t.unsqueeze(-1).float()\n",
    "        return self.time_proj(t)  # [B, n_channels]\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"A convolutional block with time conditioning and residual connections.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, time_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.time_mlp = nn.Linear(time_channels, out_channels)\n",
    "        self.use_residual = in_channels == out_channels\n",
    "        if not self.use_residual:\n",
    "            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "    def forward(self, x, t):\n",
    "        residual = x if self.use_residual else self.residual_conv(x)\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        # Broadcast time embedding over spatial dimensions.\n",
    "        h += self.time_mlp(t)[:, :, None, None]\n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        return h + residual\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-attention module to capture long-range dependencies.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.mha = nn.MultiheadAttention(channels, num_heads=4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W] -> flatten spatial dims -> [B, HW, C]\n",
    "        size = x.shape[-2:]\n",
    "        x_flat = x.flatten(2).transpose(1, 2)\n",
    "        x_norm = self.ln(x_flat)\n",
    "        attn_out, _ = self.mha(x_norm, x_norm, x_norm)\n",
    "        attn_out = attn_out + x_flat  # Skip connection\n",
    "        ff_out = self.ff_self(attn_out) + attn_out\n",
    "        # Restore spatial dims.\n",
    "        return ff_out.transpose(1, 2).view(-1, self.channels, *size)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention module to attend between source and context features.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.mha = nn.MultiheadAttention(channels, num_heads=4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_cross = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "    def forward(self, x, context):\n",
    "        # x (query) and context (key/value) both: [B, C, H, W]\n",
    "        size = x.shape[-2:]\n",
    "        x_flat = x.flatten(2).transpose(1, 2)\n",
    "        context_flat = context.flatten(2).transpose(1, 2)\n",
    "        x_norm = self.ln(x_flat)\n",
    "        attn_out, _ = self.mha(x_norm, context_flat, context_flat)\n",
    "        attn_out = attn_out + x_flat  # Skip connection\n",
    "        ff_out = self.ff_cross(attn_out) + attn_out\n",
    "        return ff_out.transpose(1, 2).view(-1, self.channels, *size)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net with optional self-attention and cross-attention for diffusion models.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        time_channels=256,\n",
    "        n_channels=64,\n",
    "        use_self_attention=True,\n",
    "        use_cross_attention=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            time_channels (int): Dimension of time embedding.\n",
    "            n_channels (int): Base number of channels.\n",
    "            use_self_attention (bool): If True, apply self-attention.\n",
    "            use_cross_attention (bool): If True, apply cross-attention in bottleneck.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        # Context projection (only matters if using cross-attention)\n",
    "        self.context_proj = nn.Conv2d(1, n_channels * 8, kernel_size=1) if use_cross_attention else Identity()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = TimeEmbedding(time_channels)\n",
    "        \n",
    "        # Encoder path\n",
    "        self.inc = ConvBlock(in_channels, n_channels, time_channels)\n",
    "        self.down1 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels, n_channels*2, time_channels),\n",
    "            SelfAttention(n_channels*2) if use_self_attention else Identity()\n",
    "        ])\n",
    "        self.down2 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels*2, n_channels*4, time_channels),\n",
    "            SelfAttention(n_channels*4) if use_self_attention else Identity()\n",
    "        ])\n",
    "        self.down3 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels*4, n_channels*8, time_channels),\n",
    "            SelfAttention(n_channels*8) if use_self_attention else Identity()\n",
    "        ])\n",
    "        \n",
    "        # Bottleneck with attention\n",
    "        self.bot1 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "        self.bot_attn = SelfAttention(n_channels*8) if use_self_attention else Identity()\n",
    "        self.cross_attn = CrossAttention(n_channels*8) if use_cross_attention else Identity()\n",
    "        self.bot2 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "        self.bot3 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        self.up1 = nn.ModuleList([\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            ConvBlock(n_channels*12, n_channels*4, time_channels),  # Concatenation: x4 (n_channels*8) and x3 (n_channels*4)\n",
    "            SelfAttention(n_channels*4) if use_self_attention else Identity()\n",
    "        ])\n",
    "        self.up2 = nn.ModuleList([\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            ConvBlock(n_channels*6, n_channels*2, time_channels),   # Concatenation: previous output (n_channels*4) with x2 (n_channels*2)\n",
    "            SelfAttention(n_channels*2) if use_self_attention else Identity()\n",
    "        ])\n",
    "        self.up3 = nn.ModuleList([\n",
    "            ConvBlock(n_channels*3, n_channels, time_channels)        # Concatenation: previous output (n_channels*2) with x1 (n_channels)\n",
    "        ])\n",
    "        \n",
    "        # Output convolution\n",
    "        self.outc = nn.Conv2d(n_channels, 1, 1)\n",
    "    \n",
    "    def forward(self, x, t, condition=None, context=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor [B, C, H, W].\n",
    "            t (torch.Tensor): Timesteps [B].\n",
    "            condition (torch.Tensor, optional): Conditioning image.\n",
    "            context (torch.Tensor, optional): Context image for cross-attention [B, 1, H, W].\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor [B, 1, H, W].\n",
    "        \"\"\"\n",
    "        # Add conditioning channel if provided.\n",
    "        if condition is not None:\n",
    "            condition = condition.expand(-1, 1, x.shape[2], x.shape[3])\n",
    "            x = torch.cat([x, condition], dim=1)\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.inc(x, t_emb)\n",
    "        x2 = self.down1[0](x1)  # MaxPool2d\n",
    "        x2 = self.down1[1](x2, t_emb)\n",
    "        x2 = self.down1[2](x2)\n",
    "        \n",
    "        x3 = self.down2[0](x2)\n",
    "        x3 = self.down2[1](x3, t_emb)\n",
    "        x3 = self.down2[2](x3)\n",
    "        \n",
    "        x4 = self.down3[0](x3)\n",
    "        x4 = self.down3[1](x4, t_emb)\n",
    "        x4 = self.down3[2](x4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x4 = self.bot1(x4, t_emb)\n",
    "        x4 = self.bot_attn(x4)\n",
    "        if context is not None:\n",
    "            # Project context to match bottleneck dimensions\n",
    "            context_proj = self.context_proj(context)\n",
    "            x4 = self.cross_attn(x4, context_proj)\n",
    "        x4 = self.bot2(x4, t_emb)\n",
    "        x4 = self.bot3(x4, t_emb)\n",
    "        \n",
    "        # Decoder\n",
    "        # Upsample x4 to match spatial dims of x3\n",
    "        x4 = F.interpolate(x4, size=x3.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x4, x3], dim=1)\n",
    "        x = self.up1[0](x)\n",
    "        x = self.up1[1](x, t_emb)\n",
    "        x = self.up1[2](x)\n",
    "        \n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up2[0](x)\n",
    "        x = self.up2[1](x, t_emb)\n",
    "        x = self.up2[2](x)\n",
    "        \n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up3[0](x, t_emb)\n",
    "        \n",
    "        return self.outc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMTrainer:\n",
    "    \"\"\"Denoising Diffusion Probabilistic Models (DDPM) Trainer.\n",
    "    Handles the training process, including:\n",
    "    - Forward/reverse diffusion processes\n",
    "    - Optimization\n",
    "    - Mixed precision training\n",
    "    - Sampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, model, n_timesteps=1000, beta_start=1e-4, beta_end=0.02,\n",
    "        lr=1e-4, device=\"cuda\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: UNet model instance\n",
    "            n_timesteps (int): Number of diffusion timesteps\n",
    "            beta_start (float): Starting noise schedule value\n",
    "            beta_end (float): Ending noise schedule value\n",
    "            lr (float): Learning rate for Adam optimizer\n",
    "            device (str): Device to run on (\"cuda\" or \"cpu\")\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.n_timesteps = n_timesteps\n",
    "        \n",
    "        # Setup noise schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, n_timesteps).to(device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # Pre-compute values for diffusion process\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def diffuse_step(self, x_0, t):\n",
    "        \"\"\"Forward diffusion step: adds noise to image according to timestep.\n",
    "        \n",
    "        Args:\n",
    "            x_0 (torch.Tensor): Original clean image\n",
    "            t (torch.Tensor): Timesteps for batch\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (noisy image, noise added)\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)  # Random noise\n",
    "        \n",
    "        # Get noise scaling factors for timestep t\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        \n",
    "        # Apply forward diffusion equation\n",
    "        x_t = sqrt_alpha_t * x_0 + sqrt_one_minus_alpha_t * noise\n",
    "        return x_t, noise\n",
    "    \n",
    "    def train_one_batch(self, x_0, condition=None, context=None):\n",
    "        \"\"\"Trains model on a single batch.\n",
    "        \n",
    "        Args:\n",
    "            x_0 (torch.Tensor): Clean images [B, C, H, W]\n",
    "            condition (torch.Tensor, optional): Conditioning information\n",
    "            context (torch.Tensor, optional): Context for cross-attention\n",
    "            \n",
    "        Returns:\n",
    "            float: Batch loss value\n",
    "        \"\"\"\n",
    "        batch_size = x_0.shape[0]\n",
    "        # Sample random timesteps for batch\n",
    "        t = torch.randint(0, self.n_timesteps, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Apply forward diffusion\n",
    "        x_t, noise = self.diffuse_step(x_0, t)\n",
    "\n",
    "        # Forward pass (removed autocast since it's not supported on MPS)\n",
    "        noise_pred = self.model(x_t, t, condition=condition, context=context)\n",
    "        # Calculate loss between predicted and actual noise\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Optimizer step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, condition=None, context=None, shape=None, n_steps=None):\n",
    "        \"\"\"Generates samples using the reverse diffusion process.\n",
    "        \n",
    "        Args:\n",
    "            condition (torch.Tensor, optional): Conditioning information\n",
    "            context (torch.Tensor, optional): Context for cross-attention\n",
    "            shape (tuple): Shape of samples to generate\n",
    "            n_steps (int, optional): Number of sampling steps\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Generated samples\n",
    "        \"\"\"\n",
    "        if n_steps is None:\n",
    "            n_steps = self.n_timesteps\n",
    "        \n",
    "        # Start from pure noise\n",
    "        x_t = torch.randn(shape, device=self.device)\n",
    "        \n",
    "        # Gradually denoise the sample\n",
    "        for t in reversed(range(n_steps)):\n",
    "            t_batch = torch.ones(shape[0], device=self.device, dtype=torch.long) * t\n",
    "            \n",
    "            # Predict noise in current sample\n",
    "            noise_pred = self.model(x_t, t_batch, condition=condition, context=context)\n",
    "            \n",
    "            # Get diffusion parameters for timestep t\n",
    "            alpha_t = self.alphas[t]\n",
    "            alpha_t_cumprod = self.alphas_cumprod[t]\n",
    "            beta_t = self.betas[t]\n",
    "            \n",
    "            # Add noise only if not the final step\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "            else:\n",
    "                noise = 0.\n",
    "            \n",
    "            # Apply reverse diffusion equation\n",
    "            x_t = (1 / torch.sqrt(alpha_t)) * (\n",
    "                x_t - beta_t / torch.sqrt(1 - alpha_t_cumprod) * noise_pred\n",
    "            ) + torch.sqrt(beta_t) * noise\n",
    "        \n",
    "        return x_t\n",
    "\n",
    "\n",
    "class MetricsTracker:\n",
    "    \"\"\"Tracks and computes various metrics during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            device: Device to run metrics computation on\n",
    "        \"\"\"\n",
    "        self.psnr = PeakSignalNoiseRatio().to(device)\n",
    "        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets all metrics for new epoch.\"\"\"\n",
    "        self.train_losses = []\n",
    "        self.psnr_scores = []\n",
    "        self.ssim_scores = []\n",
    "    \n",
    "    def update(self, pred, target, loss=None):\n",
    "        \"\"\"Updates metrics with new batch results.\"\"\"\n",
    "        if loss is not None:\n",
    "            self.train_losses.append(loss)\n",
    "        if pred is not None and target is not None:\n",
    "            self.psnr_scores.append(self.psnr(pred, target).item())\n",
    "            self.ssim_scores.append(self.ssim(pred, target).item())\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Returns average metrics for the current period.\"\"\"\n",
    "        return {\n",
    "            'loss': np.mean(self.train_losses) if self.train_losses else 0,\n",
    "            'psnr': np.mean(self.psnr_scores) if self.psnr_scores else 0,\n",
    "            'ssim': np.mean(self.ssim_scores) if self.ssim_scores else 0\n",
    "        }\n",
    "\n",
    "def visualize_samples(t1_real, t2_real, t1_gen, t2_gen, epoch, step, save=True):\n",
    "    \"\"\"Creates visualization grid of real and generated images.\n",
    "    \n",
    "    Args:\n",
    "        t1_real, t2_real: Real T1 and T2 images\n",
    "        t1_gen, t2_gen: Generated T1 and T2 images\n",
    "        epoch (int): Current epoch\n",
    "        step (int): Current step\n",
    "        save (bool): Whether to save the plot\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
    "    \n",
    "    # Real T1\n",
    "    axes[0].imshow(t1_real[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[0].set_title('Real T1')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Real T2\n",
    "    axes[1].imshow(t2_real[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[1].set_title('Real T2')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Generated T2 from T1\n",
    "    axes[2].imshow(t2_gen[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[2].set_title('T1→T2')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Generated T1 from T2\n",
    "    axes[3].imshow(t1_gen[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[3].set_title('T2→T1')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'../visualizations/diffusion/samples_epoch{epoch}_step{step}.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 577 paired T1/T2 datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13eafc349dda4f5baf8c0d5056220d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Summary:\n",
      "Average Loss: 0.1508\n",
      "Average PSNR: -3.46\n",
      "Average SSIM: 0.0145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a44d33decd64af09ac4f1b88f5d9cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Average Loss: 0.0382\n",
      "Average PSNR: 11.64\n",
      "Average SSIM: 0.0387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc90931043145f28416694d94ff3944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Average Loss: 0.0338\n",
      "Average PSNR: 11.71\n",
      "Average SSIM: 0.0457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad364682742e4a39ae15f39a435f32aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Average Loss: 0.0275\n",
      "Average PSNR: 12.73\n",
      "Average SSIM: 0.0492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8b88199c354b7397873c12c1b2f5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Average Loss: 0.0253\n",
      "Average PSNR: 13.47\n",
      "Average SSIM: 0.0740\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b46005246d645eaa25b2b8fe52dd09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Average Loss: 0.0216\n",
      "Average PSNR: 13.15\n",
      "Average SSIM: 0.0639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c4145b3bf64d5f9cb8856faacdbb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Average Loss: 0.0214\n",
      "Average PSNR: 13.26\n",
      "Average SSIM: 0.0794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013c70ce9ee44061af04716fbb09419f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Average Loss: 0.0172\n",
      "Average PSNR: 14.13\n",
      "Average SSIM: 0.0963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1703b092fe57449b803da17ddb2aa6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Average Loss: 0.0216\n",
      "Average PSNR: 13.48\n",
      "Average SSIM: 0.0872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9992f5d3524eed80ab3356a1182380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Average Loss: 0.0164\n",
      "Average PSNR: 13.52\n",
      "Average SSIM: 0.0998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3edaade3bd749b0a6b5a8f3f5e79c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "Average Loss: 0.0161\n",
      "Average PSNR: 13.79\n",
      "Average SSIM: 0.1165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3376fefddb044c485a73215b4ef803c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 Summary:\n",
      "Average Loss: 0.0183\n",
      "Average PSNR: 14.02\n",
      "Average SSIM: 0.1110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739c05ed3a4040e6bec22c8d9e981cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 Summary:\n",
      "Average Loss: 0.0186\n",
      "Average PSNR: 13.80\n",
      "Average SSIM: 0.0845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d822eea4b459422c802ee680bab92c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 Summary:\n",
      "Average Loss: 0.0171\n",
      "Average PSNR: 13.80\n",
      "Average SSIM: 0.1370\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f20d3b08a7e471c803bae6cbea8781d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Summary:\n",
      "Average Loss: 0.0160\n",
      "Average PSNR: 13.05\n",
      "Average SSIM: 0.1258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda00eb7840e476093a14214a2d2d544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Summary:\n",
      "Average Loss: 0.0171\n",
      "Average PSNR: 13.21\n",
      "Average SSIM: 0.1312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ff01edf09e414b8f375a0cbc969735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 Summary:\n",
      "Average Loss: 0.0162\n",
      "Average PSNR: 12.46\n",
      "Average SSIM: 0.1123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680923bbb2204ac9b591540df5128c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 Summary:\n",
      "Average Loss: 0.0160\n",
      "Average PSNR: 13.57\n",
      "Average SSIM: 0.1320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d675d92dcf4918b457657aa0404070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 Summary:\n",
      "Average Loss: 0.0161\n",
      "Average PSNR: 13.10\n",
      "Average SSIM: 0.1113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967014ed1904475ea904bb0793b6b463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 Summary:\n",
      "Average Loss: 0.0135\n",
      "Average PSNR: 14.65\n",
      "Average SSIM: 0.1869\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5903c7a80e40fdb108aaa7f57fd727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 Summary:\n",
      "Average Loss: 0.0147\n",
      "Average PSNR: 13.69\n",
      "Average SSIM: 0.1537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3bb57244ef4e74a9fc04b864c075c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 Summary:\n",
      "Average Loss: 0.0154\n",
      "Average PSNR: 13.63\n",
      "Average SSIM: 0.1686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edf8e8e308844c496f8af99e42bb0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 Summary:\n",
      "Average Loss: 0.0142\n",
      "Average PSNR: 13.82\n",
      "Average SSIM: 0.1557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2875f437f2401db78bf07fb0a006e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 Summary:\n",
      "Average Loss: 0.0142\n",
      "Average PSNR: 12.75\n",
      "Average SSIM: 0.1339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c29300cd9044a3a98dbe54f368430e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 Summary:\n",
      "Average Loss: 0.0148\n",
      "Average PSNR: 14.34\n",
      "Average SSIM: 0.1584\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958dd5f572f74e9ba3cb6a4269828888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 Summary:\n",
      "Average Loss: 0.0139\n",
      "Average PSNR: 14.55\n",
      "Average SSIM: 0.1479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e28f4941964bd7bcec952e73715bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 Summary:\n",
      "Average Loss: 0.0139\n",
      "Average PSNR: 13.66\n",
      "Average SSIM: 0.1530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f3bedfe5084cc79a4b96afaec02ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 Summary:\n",
      "Average Loss: 0.0144\n",
      "Average PSNR: 14.24\n",
      "Average SSIM: 0.1261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ccde63a59e47fa9053d0f411017db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 Summary:\n",
      "Average Loss: 0.0134\n",
      "Average PSNR: 14.53\n",
      "Average SSIM: 0.1064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfc33d14a1044548339ece58f69e244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 Summary:\n",
      "Average Loss: 0.0134\n",
      "Average PSNR: 13.55\n",
      "Average SSIM: 0.1556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53936e87d2594bfc92ee036c72a413c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 Summary:\n",
      "Average Loss: 0.0147\n",
      "Average PSNR: 13.55\n",
      "Average SSIM: 0.1742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c974e5a3f3014644902f14d89cf3011a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 31:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 Summary:\n",
      "Average Loss: 0.0128\n",
      "Average PSNR: 14.51\n",
      "Average SSIM: 0.1676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5ba8ab1dd64f14af703f76857d83ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 32:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 224\u001b[0m\n\u001b[0;32m    221\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m \u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 134\u001b[0m, in \u001b[0;36mtrain_diffusion\u001b[1;34m()\u001b[0m\n\u001b[0;32m    131\u001b[0m metrics\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    132\u001b[0m epoch_pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_pbar):\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    137\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 131\u001b[0m, in \u001b[0;36mMRIT1T2Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    128\u001b[0m t1_file, t2_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files[idx]\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Load T1 volume and its statistics\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m t1_vol, t1_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_and_validate_volume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_t1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaired:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Load matching T2 volume for paired data\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     t2_vol, t2_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_and_validate_volume(t2_file, is_t1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[7], line 71\u001b[0m, in \u001b[0;36mMRIT1T2Dataset._load_and_validate_volume\u001b[1;34m(self, filename, is_t1)\u001b[0m\n\u001b[0;32m     68\u001b[0m     vol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[filepath]\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Load volume using NiBabel\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     vol \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Validate volume\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_valid_volume(vol):\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\nibabel\\dataobj_images.py:374\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[1;34m(self, caching, dtype)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[1;32m--> 374\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\nibabel\\arrayproxy.py:454\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\NEU_CSYE_7374_GPU\\lib\\site-packages\\nibabel\\arrayproxy.py:423\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[1;34m(self, dtype, slicer)\u001b[0m\n\u001b[0;32m    421\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_unscaled(slicer\u001b[38;5;241m=\u001b[39mslicer), scl_slope, scl_inter)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpromote_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scaled\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class containing all training parameters and paths.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        self.t1_dir = \"../data/IXI_T1\"  # Go up one level from notebooks to root\n",
    "        self.t2_dir = \"../data/IXI_T2\"\n",
    "        \n",
    "        # Model parameters\n",
    "        self.in_channels = 2  # Image + condition channel\n",
    "        self.time_channels = 256\n",
    "        self.n_channels = 64\n",
    "        self.n_timesteps = 1000\n",
    "        self.beta_start = 1e-4\n",
    "        self.beta_end = 0.02\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 1\n",
    "    \n",
    "        self.num_epochs = 100\n",
    "        self.lr = 1e-4\n",
    "        self.save_interval = 100  # Save checkpoints every N steps\n",
    "        \n",
    "        # Directories setup - go up one level from notebooks to root\n",
    "        self.checkpoint_dir = Path(\"../checkpoints/diffusion\")\n",
    "        self.vis_dir = Path(\"../visualizations/diffusion\") \n",
    "        self.log_dir = Path(\"../logs/diffusion\")\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Cell 8: Training Functions\n",
    "\n",
    "def load_checkpoint(trainer, checkpoint_path):\n",
    "    \"\"\"Loads model and optimizer state from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        trainer: DDPMTrainer instance\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (epoch, global_step)\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['global_step']\n",
    "\n",
    "def save_checkpoint(trainer, epoch, global_step):\n",
    "    \"\"\"Saves model and optimizer state to checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        trainer: DDPMTrainer instance\n",
    "        epoch: Current epoch\n",
    "        global_step: Current global step\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': trainer.model.state_dict(),\n",
    "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "    }\n",
    "    torch.save(checkpoint, \n",
    "               config.checkpoint_dir / f'model_epoch{epoch}_step{global_step}.pt')\n",
    "\n",
    "# Main Training Loop\n",
    "\n",
    "def train_diffusion():\n",
    "    \"\"\"Main training function that handles the complete training pipeline.\"\"\"\n",
    "    \n",
    "    # Initialize tensorboard writer\n",
    "    writer = SummaryWriter(config.log_dir)\n",
    "    \n",
    "    # Initialize dataset and dataloader\n",
    "    dataset = MRIT1T2Dataset(\n",
    "        t1_dir=config.t1_dir,\n",
    "        t2_dir=config.t2_dir,\n",
    "        slice_mode='middle',\n",
    "        paired=True\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Initialize model and trainer\n",
    "    model = UNet(\n",
    "        in_channels=config.in_channels,\n",
    "        time_channels=config.time_channels,\n",
    "        n_channels=config.n_channels,\n",
    "        use_self_attention=False,\n",
    "        use_cross_attention=False,\n",
    "    )\n",
    "    \n",
    "    trainer = DDPMTrainer(\n",
    "        model=model,\n",
    "        n_timesteps=config.n_timesteps,\n",
    "        beta_start=config.beta_start,\n",
    "        beta_end=config.beta_end,\n",
    "        lr=config.lr,\n",
    "        device=config.device\n",
    "    )\n",
    "    \n",
    "    # Initialize metrics tracker\n",
    "    metrics = MetricsTracker(config.device)\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    if config.checkpoint_dir.exists():\n",
    "        checkpoints = list(config.checkpoint_dir.glob('model_epoch*_step*.pt'))\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "            # Load checkpoint with weights_only=True to avoid security warning\n",
    "            start_epoch, global_step = load_checkpoint(trainer, latest_checkpoint, weights_only=True)\n",
    "            print(f\"Resuming from epoch {start_epoch}, step {global_step}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        metrics.reset()\n",
    "        epoch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(epoch_pbar):\n",
    "            # Move data to device\n",
    "            t1 = batch['T1'].to(config.device)\n",
    "            t2 = batch['T2'].to(config.device)\n",
    "            \n",
    "            # Train T1 -> T2\n",
    "            loss_t1_t2 = trainer.train_one_batch(\n",
    "                x_0=t2,            # Target is T2\n",
    "                condition=t1,      # Condition on T1\n",
    "                context=t1         # Cross-attention sees T1\n",
    "            )\n",
    "            \n",
    "            # Train T2 -> T1\n",
    "            loss_t2_t1 = trainer.train_one_batch(\n",
    "                x_0=t1,            # Target is T1\n",
    "                condition=t2,      # Condition on T2\n",
    "                context=t2         # Cross-attention sees T2\n",
    "            )\n",
    "            \n",
    "            # Update metrics\n",
    "            avg_loss = (loss_t1_t2 + loss_t2_t1) / 2\n",
    "            metrics.update(None, None, avg_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_pbar.set_postfix({\n",
    "                'loss': f\"{avg_loss:.4f}\",\n",
    "                'step': global_step\n",
    "            })\n",
    "            \n",
    "            # Checkpoint and visualization\n",
    "            if global_step % config.save_interval == 0:\n",
    "                trainer.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Generate samples\n",
    "                    t2_gen = trainer.sample(\n",
    "                        condition=t1,\n",
    "                        context=t1,\n",
    "                        shape=t1.shape\n",
    "                    )\n",
    "                    t1_gen = trainer.sample(\n",
    "                        condition=t2,\n",
    "                        context=t2,\n",
    "                        shape=t2.shape\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate metrics for generated images\n",
    "                    metrics.update(t2_gen, t2)\n",
    "                    metrics.update(t1_gen, t1)\n",
    "                    \n",
    "                    # Visualize samples\n",
    "                    visualize_samples(\n",
    "                        t1, t2,\n",
    "                        t1_gen, t2_gen,\n",
    "                        epoch, global_step\n",
    "                    )\n",
    "                    \n",
    "                    # Log to tensorboard\n",
    "                    current_metrics = metrics.get_metrics()\n",
    "                    writer.add_scalar('Loss/train', current_metrics['loss'], global_step)\n",
    "                    writer.add_scalar('Metrics/PSNR', current_metrics['psnr'], global_step)\n",
    "                    writer.add_scalar('Metrics/SSIM', current_metrics['ssim'], global_step)\n",
    "                    writer.add_images('Samples/T1', t1, global_step)\n",
    "                    writer.add_images('Samples/T2', t2, global_step)\n",
    "                    writer.add_images('Samples/T1_generated', t1_gen, global_step)\n",
    "                    writer.add_images('Samples/T2_generated', t2_gen, global_step)\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    save_checkpoint(trainer, epoch, global_step)\n",
    "                \n",
    "                trainer.model.train()\n",
    "            \n",
    "            global_step += 1\n",
    "        \n",
    "        # End of epoch\n",
    "        epoch_metrics = metrics.get_metrics()\n",
    "        print(f\"\\nEpoch {epoch} Summary:\")\n",
    "        print(f\"Average Loss: {epoch_metrics['loss']:.4f}\")\n",
    "        print(f\"Average PSNR: {epoch_metrics['psnr']:.2f}\")\n",
    "        print(f\"Average SSIM: {epoch_metrics['ssim']:.4f}\")\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Start training\n",
    "    train_diffusion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEU_CSYE_7374_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
